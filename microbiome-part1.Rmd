---
title: "Getting Started with Microbiome Data Analysis in R"
# subtitle: "Integrated Microbiome Data Analysis Workflows"
author: "Teresia Mrema-Buza, A Microbiome Computational Scientist and Owner of the Complex Data Insights, LLC, USA"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
knit: bookdown::render_book
output:
  bookdown::bs4_book:
    includes:
      in_header:
        
  bookdown::gitbook:
    includes:
      in_header: header.html
    config: 
      toc:
       collapse: 
      search: yes
      fontsettings:
        size: 2
    split_by: section
    split_bib: no
    df_print: paged
    number_sections: true
    css:
    - css/style.css
    - style.css
    
  html_document:
    code_folding: hide
    
  bookdown::html_document2:
    code_folding: hide
    
  bookdown::pdf_book:
    config:
      toc = true
      toc_depth = 2
      number_sections = true
      fig_caption = true
      keep_tex = no
      pandoc_args = null
      toc_unnumbered = no
      toc_appendix = no
      toc_bib = no 
      quote_footer = null
      highlight_bw = true
      latex_engine = xelatex
      df_print = kable
      base_format = rmarkdown::pdf_document
  includes:
    in_header: 
    - latex/header.tex
    - latex/preamble.tex
documentclass: book
classoption: openany #remove empty pages in pdf doc
bibliography:
- library/book.bib
- library/packages.bib
- library/microbiome.bib
- library/software.bib 
citation_package:
- natbib
- biblatex
- amsplain
colorlinks: true
css:
- css/style.css
always_allow_html: true
fig_caption: true
fontsize: 12pt
geometry: margin=1in
indent: false
keep_tex: true
link-citations: true
mainfont: Times New Roman
biblio-style: apalike
spacing: double
header-includes: 
- \usepackage{setspace}
- \newpage
- \newenvironment{tmbinfo}[0]{}{}
- \renewenvironment{tmbinfo}[0]{}{}
- \newenvironment{tmbalert}[0]{}{}
- \renewenvironment{tmbalert}[0]{}{}
- \newenvironment{tmbshare}[0]{}{}
- \renewenvironment{tmbshare}[0]{}{}
description: |
  This is a practical user's guide for **Systematic Microbiome Data Analysis in R**. The guide provides integrated and highly curated solutions for achieving better results.
---


```{r pkgbiblib, include=FALSE}
knitr::write_bib(c(
  .packages(), 'base','bookdown','rmarkdown','tidyverse','shiny','vegan','data.table, dendextend, robCompositions, microbiome, ALDEx2, caret, rms, phyloseq'
), 'library/packages.bib')
```

```{r setup, include=FALSE}
source(file = "R/common.R")
```

# Getting Started with Microbiome Data Analysis in R {-#frontpage}

```{r include=FALSE}
library(DiagrammeR)
library(DiagrammeRsvg)
library(rsvg)

# 1. Make a graph
graph <- DiagrammeR::grViz("
digraph {
graph [layout = dot, rankdir = TD]

node [
shape = box, 
style = filled, 
fillcolor = white, 
fontname = Helvetica,
penwidth = 2.0] 

edge [arrowhead = diamond]

A [label = 'Getting Started', fillcolor = white, penwidth = 5.0]
B [label = 'Requirements']
C [label = 'Basic Software']
D [label = 'Sample Metadata']
E [label = 'Sequencing Data']
F [label = 'Analysis & Visualization Tools']
G [label = 'Preprocessing Tools']
H [label = 'Bioinformatics Pipelines']
I [label = 'Mapping Files']

{A}  -> B
{B}  -> C
{B}  -> D
{B}  -> E
{C}  -> F
{C}  -> G
{C}  -> H
{B}  -> I

}", height = 400, width = 600)

# 2. Convert to SVG, then save as png
part1 = DiagrammeRsvg::export_svg(graph)
part1 = charToRaw(part1) # flatten
rsvg::rsvg_png(part1, "img/part1.png")
```


<a href=""><img src="images/smda_part1.png" alt="Book cover" width="100%" style="padding: 50px 0px 100px 0px; float: right;"/></a>

## Quick Glimpse {-}
Investigating the role of microbial communities in health and disease requires a thorough knowledge of the entire analytical process. Using wrong approaches can cost a significant amount of dollars and lengthy process to achieve the desired results. This is <b>PART 1</b> of the practical user guides intended to provide analytical support to the microbiome research community. The entire guide is reproducible, allowing users to easily follow along. If interested, user may use this model to publish their findings in a book format.

## Structure of this guide {-}
This guide is divided into chapters to facilitate easy navigation. Each chapter contains several sections as displayed in the navigation bars on the left and right. Click the hyper-linked text if you want to jump into a specific chapter or section.

## Code availability {-}
The code is available at a public [GitHub repository](https://github.com/tmbuza/microbiome-part1/). If interested you can request a consulting service by contacting the developer of this repo using <a href="https://complexdatainsights.com/contact-us">this contact form</a>. 

<!--chapter:end:index.Rmd-->

# (PART) BASIC SOFTWARE {-}

# Installing R Environment {#R-environment}

<a href=""><img src="images/rlogoblue.png" alt="R Software" width="100" style="padding: 0 15px; float: left;"/></a>
**R** is a free software for statistical computing, data analysis, and graphics [@RCoreTeam2021]. We need to install R application on a personal computer to process the R programming language. You can download and install R using these steps:

1. Go to [https://www.r-project.org/](https://www.r-project.org/).
2. On the left, under Download, click on [CRAN](http://cran.r-project.org/mirrors.html) to access the mirrors. CRAN (Comprehensive R Archive Network) is mirrored on nearly 100 registered servers in nearly 50 regions world. See [CRAN mirror status](https://cran.r-project.org/mirmon_report.html).
3. https://cloud.r-project.org/ Pick a mirror that is close to your location, and automatically R will connect to that server ready to download the package files.
4. Select a compatible platform to download precompiled binary distributions of the base system, which also come with contributed packages.

<br>

# Installing RStudio Environment {#rstudio-ide}

<a href=""><img src="images/rstudio.png" alt="RStudio" width="100" style="padding: 0 15px; float: left;"/></a>

**RStudio** is a free program that integrates with R as an IDE (Integrated Development Environment) to implement most of the analytical functionalities [@RStudioTeam2021].  For effective analysis, we must install R before installing RStudio. We will intensively use RStudio IDE to give us a user interface. We are interested in **RStudio Desktop**, which is the open-source regular desktop application. You can install it like this:

1. Go to [https://rstudio.com/products/rstudio/](https://rstudio.com/products/rstudio/).
2. Click on [RStudio Desktop](https://rstudio.com/products/rstudio/#rstudio-desktop) box to move to the open source edition.
3. Choose your preferred [license](https://rstudio.com/products/rstudio/download/) either open source or commercial.
4. Select [installer](https://rstudio.com/products/rstudio/download/#download) compatible to your operating system.

<br>

![Screen shot of RStudio User Interface](images/RStudioIDE.png){ width=100% }

<br>

# Installing R packages {#install-packages}
Install basic packages that get the analysis started. We will introduce additional packages within the subsequent sections whenever needed


| Package | Description |
| :--------------------- | :---------------------------------------------- |
| [tidyverse](https://www.rdocumentation.org/packages/tidyverse/) | The *tidyverse* package is a collection of R packages designed with common APIs. It includes *readr*, *dplyr*, *tidyr*, *ggplot2*, *tibble*, *purr*, etc. |
| [readr](https://www.rdocumentation.org/packages/readr/) | The *readr* package turns flat files into data frames |
| [dplyr](https://www.rdocumentation.org/packages/dplyr/) | The *dplyr* package focuses on data frames. We will use dplyr (a lot) to subset, summarize, rearrange, and to join together data sets. |
| [tidyr](https://www.rdocumentation.org/packages/tidyr/) | The *tidyr* package converts data into the tidy format where each variable is a column, each observation is a row, and each type of observational unit is a table. |
| [ggplot2](https://www.rdocumentation.org/packages/ggplot2/) | The *ggplot2* functions create elegant graphics based on the Grammar of Graphics. |
| [knitr](https://www.rdocumentation.org/packages/knitr/) | The *knitr* package is excellect for integrating R code into different forms of Literate Programming to generate dynamic reports. |
| [magrittr](https://www.rdocumentation.org/packages/magrittr/) | The *magrittr* provides an operator ( %>% ) for chaining several commands. |
| [purr](https://www.rdocumentation.org/packages/purrr/) | The *purr* provides tools for working with functions and vectors. |
| [forcat](https://www.rdocumentation.org/packages/forcats/) | The *forcats* provide functions for solving common problems with factors. |



## Location of R packages used in this Book

* [CRAN](https://cran.r-project.org/) (Comprehensive R Archive Network): The official repository. Reviewed!
* [Bioconductor](https://www.bioconductor.org/): Most popular repository for bioinformatics software. Reviewed!
* [Github](https://github.com/): Most popular repository for open source projects. Not Reviewed!



## Installing and loading packages from CRAN
```{}
install.packages("packagename")
library("packagename")
```


## Installing and loading R packages from Bioconductor
```{}
# Install BiocManager
if (!requireNamespace("BiocManager", quietly = TRUE)) install.packages("BiocManager")
library("BiocManager")

# Then install packages like this:
BiocManager::install("packagename")}
library("packagename")
```


## Installing and loading R packages from GitHub

```{}
# Install devtools from CRAN
install.packages("devtools")
library(devtools)

# Or the development version from GitHub:
devtools::install_github("r-lib/devtools")
library(devtools)

# Then install any GitHub package like this:

devtools::install_github("githubID/packagename")
library("packagename")
```


# Getting Help in R

## Show basic package information 

```{}
help("packagename")

or

?packagename

help(package = "packagename")

```

## Getting a general overview
- of a function, including an example code**

```{}
help(functionname)

or

help(functionname, package = "packagename")

```

## Remove unneeded packages from your computer 

```{}
remove.packages("packagename")
```

## Unload a package 

```{}
detach("package::packagename")
```

## Citation Information
Run on R console command line
```{}
# Citing R\n
citation()

# Citing RStudio
library(rstudioapi)
versionInfo()$citation

# Citing Package ...example
citation("tidyverse")
```

## Useful links

1. R Packages: [https://www.datacamp.com/community/tutorials/r-packages-guide](https://www.datacamp.com/community/tutorials/r-packages-guide).
2. Search CRAN, Bioconductor and GitHub packages and functions: [https://www.rdocumentation.org/](https://www.rdocumentation.org/).



<!--chapter:end:01a_basic_software.Rmd-->

# (PART) SAMPLE METADATA {-}

# Microbiome Sample Metadata {#sample-metadata}

## What is metadata?
* <b>Metadata</b> is a set of data that describes and provides information about other data. It is commonly defined as **data about data**.
* **Sample metadata** described in this book refers to the description and context of the individual sample collected for a specific microbiome study.

## Metadata structure
* Metadata collected at different stages (Figure 1) are typically organized in an Excel or Google spreadsheet where:
  * The metadata table columns represent the properties of the samples.
  * The metadata table rows contain information associated with the samples.
  * Typically, the first column of sample metadata is Sample ID, which designates the key associated to individual sample
  * Sampl ID must be unique.

## Embedded metadata
* In most cases, you will find the metadata detached from the experimental data.
* Embedded metadata integrates the experimental data especially for graphics.
* Major microbiome analysis platforms require sample metadata, commonly referred to as **mapping file** when performing downstream analysis.

<br>

# Sample Metadata profiling {#metadata-profiling}
```{block, type="tmbinfo", echo=TRUE}
Typically, after sequencing the microbiome DNA, the investigators are encouraged to deposit the sequence reads in a public repository. The Sequence Read Archive (SRA) is currently the best bioinformatics database for read information. The good thing about SRA is that it integrates data from the NCBI, the European Bioinformatics Institute (EBI), and the DNA Data Bank of Japan (DDBJ). 

```

Here we profile metadata associated with the bushmeat microbiome bioproject number [PRJNA477349](https://www.ncbi.nlm.nih.gov/sra?linkname=bioproject_sra_all&from_uid=477349).

- We will demonstrate:
  - how to download published sample metadata from the NCBI-SRA archive using the bioproject number as a primary search key. 
  - how to load the metadata file into R environment. 
  - how to manipulate and create a tidy metadata.
  - how to filter and select desired metadata for integrating with downstream analyses.
  
## Downloading metadata using the NCBI-SRA run selector
- In this demo we will save the metadata in a folder names `data`. So, let's create the folder (if it doesn't exist!). 

```{r}
if (!dir.exists("data")) {dir.create("data")}
```

> Note that the SRA filename for metadata is automatically named `SraRunTable.txt`...see the figure below for more guidance.


![Screen shot of SRA Run Selector for metadata associated with the NCBI-SRA bioproject number PRJNA477349](images/sra_run_selector.png){width=100%}

<br>

## Loading metadata into R environment
```{r warning=FALSE}
library(tidyverse)
sraruntable <- read_delim("data/SraRunTable.txt", show_col_types = FALSE)
```

## How many rows and columns
```{r}
paste("There are", dim(sraruntable)[1], "rows and", dim(sraruntable)[2],"columns in this metadata")
```

## Column names
Getting a clear knowledge about the variables associated with a sample metadata can help in filtering the most important features.
 
```{r}
colnames(sraruntable)
```

> Note that a fraction of the columns may be needed to answer all the research questions. 


## Tidying and subsetting metadata
- We want the sample metadata to include a few desired variables. 
- In this example we desire to include sample collection point i.e latitude and longitude. 
- We will drop row with `NA` in location column.
- It is a good habit to rename, modify or replace longer column names with meaningful names.
- We will select a few columns to create a desired metadata for downstream analyses.
- Saving the tidy metadata file in `RDS or RData` format will preserve a compressed file.

### Getting s desired metadata
```{r}
metadata <- sraruntable %>%  
  rename_all(tolower) %>% 
  rename(sample_id = run) %>% 
  drop_na(lat_lon) %>% 
  mutate(
    geo_loc_name = str_replace_all(geo_loc_name, "Tanzania: ", ""),
    geo_loc_name = str_replace_all(geo_loc_name, "The Greater Serengeti Ecosystem", "Serengeti"),
    geo_loc_name = str_replace_all(geo_loc_name, " Ecosystem", ""),
    isolate = str_replace_all(isolate, "_\\d*$", ""),
    lat_lon = str_replace_all(lat_lon, " E$", ""),
    latitude = as.numeric(str_replace_all(lat_lon, " S.*", "")) * -1,
    longitude = as.numeric(str_replace_all(lat_lon, ".*S ", ""))) %>% 
  rename(ecosystem = geo_loc_name) %>%
  rename(description = host) %>% 
  mutate(bases = round(bases/1E6, digits = 0)) %>% 
  select(sample_id, ecosystem, isolate, latitude, longitude, milionbases=bases, description)

saveRDS(metadata, "RDataRDS/metadata.rds")
```

### Compact structure of the tidy metadata
```{r}
str(metadata)
```


### Any missing values? 
The `df_status()` from funModeling R package is excellent for quick inspecting the variables, missing values, data type and total unique variables.

```{r}
library(funModeling)
metadata <- readRDS("RDataRDS/metadata.rds")
df_status(metadata)
```

Key: **q_zeros**: quantity of missing data; **p_zeros**: percentage of missing data, **q_na**: quantity of NA; **p_na**: percentage of NA, **q_inf**: quantity of infinite values; **p_inf**: percentage of infinity values,  **type**: factor, character, integer or numeric; **unique**: levels of the variable.


### Graphical view of variable frequency
- We can also use the `freq()` function from the funModeling R package to get a clear view of the variables and unique counts.

```{r var_freq}
freq(metadata, input = c("ecosystem", "isolate"))
```


<br>


### Which samples have the smallest number of reads?
- We want to use samples with smaller number of reads for computation reasons.
- Read depth is proportional to the number of bases (renamed to milionbases)
- Let's explore read depth across isolates and ecosystem.
- We will sort the bases in ascending order.

```{r fig.height=7, fig.width=7}
metadata %>% 
  select(isolate, ecosystem, milionbases) %>% 
  arrange(milionbases)
  
```

> Looks like the top samples with smallest number of reads are from same ecosystem.


<br>

### Distribution across a common variable
> Note that we use `facet_grid()` function to partition the plot into panels to show different subsets of the data. This can help to reveal the distribution of levels and their relationship.

```{r distrib}
metadata %>% 
  ggplot(aes(x = isolate, y = milionbases, fill = ecosystem)) +
  facet_grid(~ ecosystem) +
  geom_col(position = "dodge") +
  coord_flip() +
  labs(x = "Animal Name", y = "Read size (milion bases)") +
  theme_light()  + 
  nolegend +
 axislayout
```

<br>

# Locating sample collection points {#sample-location}
## Interactive map 
- The [`leaflet`](https://rstudio.github.io/leaflet/) R package can do a great job in dropping a pin on the corresponding coordinate. 
- Note that samples collected on the same coordinate will overlap. 
- You can zoom in-out to expand or minimize the map. 
- You can also mouse over the pin to see the variable label. 

Let's give it a try. 

```{r map}
library(leaflet)
library(leaflet.esri)
library(leaflet.providers)
library(leaflet.extras)
library(data.table)
library(dplyr)

minLat <- min(metadata$latitude) - 0.1
minLon <- min(metadata$longitude) + 0.1
maxLat <- max(metadata$latitude) + 0.1
maxLon <- max(metadata$longitude) + 0.1

metadata %>%
  leaflet() %>% 
  addProviderTiles(providers$Esri.NatGeoWorldMap) %>%
  fitBounds(minLon, minLat, maxLon, maxLat) %>%
  addMarkers(lng = ~longitude, lat = ~latitude, popup = ~isolate, label = ~ c(isolate)) %>%
  addCircles(color="magenta", radius = log1p(metadata$longitude) * 10)
```

<br>

## Example of screenshots from zoom in-out

![World map: Geographical location of sample collection points.](images/loc_on_world_map.png){ width=60% }

<br>

![Continent map: Geographical location of sample collection points.](images/loc_on_continent_map.png){ width=60% }

<br>

![Country map: Geographical location of sample collection points.](images/loc_on_country_map.png){ width=60% }

<br>

![Site map: Geographical location of sample collection points.](images/loc_on_site_map.png){ width=60% }


```{r include=FALSE}
save(
  metadata,

  file = "RDataRDS/saved_objects.RData")
```

<!--chapter:end:01b_samplemetadata.Rmd-->

# (PART) PREPROCESSING TOOLS {-}

# Required Processing Tools {#preprocessing-tools}

There several tools out there that can help in preprocessing raw read. Listed below are some of most common tools used in understanding the characteristics of the read and their quality scores. Click on the hyperlinked tool to learn more how to install it.

- [Seqkit](https://bioinf.shenwei.me/seqkit/download/) [@seqkit2016]
- [FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) [@fastqc2018]
- [MultiQC](https://multiqc.info/) [@multiqc2016]
- [BBMap](https://sourceforge.net/projects/bbmap/files/latest/download) platform [@bbmap2016] 
- [Kneaddata](https://huttenhower.sph.harvard.edu/kneaddata/) in biobakery platform [@kneaddata2022] 
 

# Installing Preprocessing Tools on Mac {#install-on-mac}

## Installing seqkit
```bash
wget --no-check-certificate https://github.com/shenwei356/seqkit/releases/download/v0.8.0/seqkit_darwin_amd64.tar.gz
tar -zxvf seqkit_darwin_amd64.tar.gz
mv seqkit code/
cp code/seqkit ~/bin/
rm seqkit_darwin_amd64.tar.gz
```

## Installing fastqc
```bash
wget --no-check-certificate https://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.7.zip
unzip fastqc_v0.11.7.zip
chmod 755 FastQC/fastqc
mv -v FastQC code/
cp code/FastQC/fastqc ~/bin/
rm fastqc_v0.11.7.zip
```

## Installing bbmap
```bash
wget --no-check-certificate https://sourceforge.net/projects/bbmap/files/BBMap_37.90.tar.gz
tar -xvzf BBMap_37.90.tar.gz
mv -v bbmap code/
cp code/bbmap/bbduk.sh ~/bin/
rm BBMap_37.90.tar.gz
```


# Installing Preprocessing Tools on Linux {#install-on-linux}

## Install seqkit
```bash
wget --no-check-certificate https://github.com/shenwei356/seqkit/releases/download/v0.8.0/seqkit_darwin_amd64.tar.gz
tar -zxvf seqkit_darwin_amd64.tar.gz
mv seqkit code/
cp code/seqkit ~/bin/
rm seqkit_darwin_amd64.tar.gz
```

## Install fastqc
```bash
wget --no-check-certificate https://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.7.zip
unzip fastqc_v0.11.7.zip
chmod 755 FastQC/fastqc
mv -v FastQC code/
cp code/FastQC/fastqc ~/bin/
rm fastqc_v0.11.7.zip
```

## Install bbmap
```bash
wget --no-check-certificate https://sourceforge.net/projects/bbmap/files/BBMap_37.90.tar.gz
tar -xvzf BBMap_37.90.tar.gz
mv -v bbmap code/
cp code/bbmap/bbduk.sh ~/bin/
rm BBMap_37.90.tar.gz
```


# Installing Preprocessing Tools on Windows {#install-on-win}

## Install seqkit
```bash
wget --no-check-certificate https://github.com/shenwei356/seqkit/releases/download/v0.8.0/seqkit_darwin_amd64.tar.gz
tar -zxvf seqkit_darwin_amd64.tar.gz
mv seqkit code/
cp code/seqkit ~/bin/
rm seqkit_darwin_amd64.tar.gz
```

## Install fastqc
```bash
wget --no-check-certificate https://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.7.zip
unzip fastqc_v0.11.7.zip
chmod 755 FastQC/fastqc
mv -v FastQC code/
cp code/FastQC/fastqc ~/bin/
rm fastqc_v0.11.7.zip
```

## Install bbmap
```bash
wget --no-check-certificate https://sourceforge.net/projects/bbmap/files/BBMap_37.90.tar.gz
tar -xvzf BBMap_37.90.tar.gz
mv -v bbmap code/
cp code/bbmap/bbduk.sh ~/bin/
rm BBMap_37.90.tar.gz
```

> Note that the links for each tool may be outdated. Make sure to check for latest instructions online.






<!--chapter:end:01c_preprocessing_tools.Rmd-->

# (PART) SEQUENCING DATA {-}

# Raw Read Fastq Sequences {#raw-data}

Read sequencing data may be obtained from different sources. The most common ones include:

1. Reads from sequencing platforms for research purposes.
2. Reads downloaded from the Sequence Read Archive (SRA).
3. Reads synthesized `in-silico` using special simulation software


## Reads from sequencing platforms
The most common sources of sequencing data are from research projects. There are multiple sequencing platforms out there. For example, Illumina sequencing company uses technologies capable of profiling entire microbial communities present in environmental. Example include: 

- 16S rRNA Sequencing
- Shotgun Metagenomic Sequencing

## Reads from archives such as NCBI-SRA
The NCBI Sequence Read Archive (SRA) stores sequencing data from the next generation sequencing platforms. Users can download data from the SRA archive. Below are minimal steps to do that:

### Install SRA Toolkit on Mac OS.
- Navigate to where you want to install the tools, preferably the home directory.
- For more information click [here](https://github.com/ncbi/sra-tools/wiki/02.-Installing-SRA-Toolkit).

```{}
curl -LO  https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/3.0.0/sratoolkit.3.0.0-mac64.tar.gz
tar -xf sratoolkit.3.0.0-mac64.tar.gz

export PATH=$HOME/sratoolkit.3.0.0-mac64/bin/:$PATH

```

### Create a cache root directory
```{}
mkdir -p ~/ncbi
echo '/repository/user/main/public/root = "/Volumes/SeagateTMB/SRA/BUSHMEAT/SRR7450"' > ~/ncbi/user-settings.mkfg
```


### Confirm sra toolkit configuration
- Command below will display a blue colored dialog.
- Use tab or click `c` to navigate to cache tab.
- Review the configuration then save `s` and exit `x`.

```{}
vdb-config -i
```


![A screenshot of the SRA configuration.](images/sra_config_cache.png){width=100%}

<br>

For more information click [here](https://github.com/ncbi/sra-tools/wiki/03.-Quick-Toolkit-Configuration).

## Downloading the fastq files
- Be sure that the `fasterq-dump` is in the path.
- Type `which fasterq-dump` to confirm if is in the path.
- Use the absolute path like `~/sratoolkit.3.0.0-mac64/bin/fasterq-dump` if the executable file is not in the path.
- The example below uses absolute path to execute the `fasterq-dump` command.

> Notice that the output and temporary files are placed in an external drive mounted to the computer.

> The sratoolkit folder is in the home directory.

```{}
for (( i = 706; i <= 761; i++ ))
	do
		time ~/sratoolkit.3.0.0-mac64/bin/fasterq-dump SRR7450$i \
		-O /Volumes/SeagateTMB/SRA/BUSHMEAT/SRR7450/ \
		-t /Volumes/SeagateTMB/SRA/BUSHMEAT/SRR7450/tmpfiles \
		--threads 4 \
		--split-3;
	done
```


# How to Synthesize Metagenomics Reads {#synthetic-reads}
- Most `insilico` data is used for testing software before using real data.
- Generating `insilico` data can be challenging but not can provide a starting data for testing some pipelines.
- Below is a demo for generating NovaSeq, MiSeq and HiSeq `insilico` sequencing data.

### Demo1: Using NovaSeq to generate metagenomics data

- Five novaseq paired fastq files

```{}
iss generate \
	--ncbi bacteria viruses archaea -U 15 0 2 \
	--coverage lognormal \
	--n_reads 0.12M \
	--model novaseq \annual report
	--output data/novaseq_01 \
	--compress \
	--seed 1
iss generate \
	--ncbi bacteria viruses archaea -U 15 1 1 \
	--coverage lognormal \
	--n_reads 0.11M \
	--model novaseq \
	--output data/novaseq_02 \
	--compress \
	--seed 2
iss generate \
	--ncbi bacteria viruses archaea -U 10 0 0 \
	--coverage lognormal \
	--n_reads 0.1M \
	--model novaseq \
	--output data/novaseq_03 \
	--compress \
	--seed 3
iss generate \
	--ncbi bacteria viruses archaea -U 10 0 0 \
	--coverage lognormal \
	--n_reads 0.1M \
	--model novaseq \
	--output data/novaseq_04 \
	--compress \
	--seed 4
iss generate \
	--ncbi bacteria viruses archaea -U 10 0 0 \
	--coverage lognormal \
	--n_reads 0.1M \
	--model novaseq \
	--output data/novaseq_05 \
	--compress \
	--seed 5
	
```	

### Demo2:  Using MiSeq to generate metagenomics data

- Five miseq paired fastq files

```{}
iss generate \
	--ncbi bacteria viruses archaea -U 10 0 2 \
	--coverage lognormal \
	--n_reads 0.12M \
	--model miseq \
	--output data/miseq_01 \
	--compress \
	--seed 16
iss generate \
	--ncbi bacteria viruses archaea -U 15 1 1 \
	--coverage lognormal \
	--n_reads 0.11M \
	--model miseq \
	--output data/miseq_02 \
	--compress \
	--seed 17
iss generate \
	--ncbi bacteria viruses archaea -U 5 0 0 \
	--coverage lognormal \
	--n_reads 0.1M \
	--model miseq \
	--output data/miseq_03 \
	--compress \
	--seed 18
iss generate \
	--ncbi bacteria viruses archaea -U 15 0 0 \
	--coverage lognormal \
	--n_reads 0.1M \
	--model miseq \
	--output data/miseq_04 \
	--compress \
	--seed 19
iss generate \
	--ncbi bacteria viruses archaea -U 5 0 0 \
	--coverage lognormal \
	--n_reads 0.1M \
	--model miseq \
	--output data/miseq_05 \
	--compress \
	--seed 20
	
```	
	
### Demo3: UsingbHiSeq to generate metagenomics data

- Five hiseq paired fastq files

```{}
iss generate \
	--ncbi bacteria viruses archaea -U 10 1 2 \
	--coverage lognormal \
	--n_reads 0.12M \
	--model hiseq \
	--output data/hiseq_01 \
	--compress \
	--seed 21
iss generate \
	--ncbi bacteria viruses archaea -U 15 1 1 \
	--coverage lognormal \
	--n_reads 0.11M \
	--model hiseq \
	--output data/hiseq_02 \
	--compress \
	--seed 22
iss generate \
	--ncbi bacteria viruses archaea -U 10 0 0 \
	--coverage lognormal \
	--n_reads 0.1M \
	--model hiseq \
	--output data/hiseq_03 \
	--compress \
	--seed 23
iss generate \
	--ncbi bacteria viruses archaea -U 10 0 0 \
	--coverage lognormal \
	--n_reads 0.1M \
	--model hiseq \
	--output data/hiseq_04 \
	--compress \
	--seed 24
iss generate \
	--ncbi bacteria viruses archaea -U 10 0 0 \
	--coverage lognormal \
	--n_reads 0.1M \
	--model hiseq \
	--output data/hiseq_05 \
	--compress \
	--seed 25

```

<br>

# How to Resize Fastq Files {#resizing-fastq}
- Sometimes we want to extract a small subset to test the bioinformatics pipeline.
- You can resize the fastq files using the `seqkit sample` function [@Shen2016; @seqkit2022].
- Below is a demo for extracting only 0.1% of the sequencing data.

```bash
mkdir -p data
for i in {706..761}
  do
    cat SRR7450$i\_R1.fastq \
    | seqkit sample -p 0.01 \
    | seqkit shuffle -o data/SRR7450$i\_R1_sub.fastq \
    | cat SRR7450$i\_R2.fastq \
    | seqkit sample -p 0.01 \
    | seqkit shuffle -o data/SRR7450$i\_R2_sub.fastq
  done

```

<!--chapter:end:01d_sequencing_data.Rmd-->

# (PART) BIOINFORMATICS PIPELINES {-}

# Microbiome Pipelines for 16S rRNA {#bioinfo-16s}
In this guide we will install or highlight tools of interest for bioinformatics analysis of 16S rRNA gene sequencing data.  Make sure to check for latest instructions online.

## Mothur Pipeline
- Mothur is an open-source software package for bioinformatics data processing.
- It is a very famous platform comprised of over 145 tools that can be integrated to for a desired pipeline. 
- Mothur has a [basic tutorial](https://mothur.org/wiki/miseq_sop/) that help users get started with 16S rRNA gene analysis. 
- We can download a stable platform from [here](https://github.com/mothur/mothur/releases/).

## QIIME2 Pipeline
- QIIME2 is an open-source microbiome analysis platform with integrated software for quality control such as DADA2.
- It is a very famous platform with an active community forum. 
- QIIME2 has [profound tutorials](https://docs.qiime2.org/2022.2/tutorials/) that help users get started with 16S rRNA gene analysis. 
- We can instal the latest version from [here](https://docs.qiime2.org/2022.2/install/).


## Nephele Platform
- Nephele is an open, accessible high performmance cloud platform platform for microbial bioinformatics. 
- Nephele integrates Mothur, QIIME2, DADA2 and Biobakery
- For more details see some of their [tutorials](https://nephele.niaid.nih.gov/user_guide/) and useful [user guide](https://nephele.niaid.nih.gov/user_guide/).


# Microbiome Pipelines for Metagenomics {#bioinfo-metagenomics}
The installation instruction of various pipeline is found [here](https://github.com/biobakery/biobakery/wiki). See the table below for specific software.

| Pipeline | Description | Tutorial |
|-----------|-----------------|----------------------|
| Keaddata | Quality controll on metagenomic sequencing data | [Tutorial](https://github.com/biobakery/biobakery/wiki/kneaddata)  |
| MetaPhlAn | Taxonomic Profiling from metagenomic shotgun sequencing data| [Tutorial](https://github.com/biobakery/biobakery/wiki/metaphlan3)  |
| HumanN  | Functional Profiling from metagenomic sequencing data | [Tutorial](https://github.com/biobakery/biobakery/wiki/humann3)  |
||| 



<!--chapter:end:01e_bioinfo_pipelines.Rmd-->

# (PART) MAPPING FILES {-}

# Preparing Mapping Files
The mapping files are require to direct the pipeline where to look for the files containing the sequencing data.

- The format of the mapping files for `mothur` and `QIIME2` pipelines is slightly different. 
- We will demonstrate how to prepare both.
- Each mapping file will contain only the files that are parsed by bioinformatics analysis. 

> We will select files containing <65,000 sequences for demo purposes. We will start by getting the statistics of the samples.

## Explore read statistics 
- Are the files compressed?
- You may want to save space by compressing the unzipped files. 
- Navigate to the folder containing the fastq files and compress them using `gzip` function.

```bash
gzip *.fastq
```

> From this point foward we will assume that all the fastq files are in `fastq.gz` format.

- Using `seqkit stat` function to compute read statistics.
- We will store the output in `data/stats1/seqkit_stats.txt`.
- The seqkit output will contain useful information for preparing the mapping files.

```bash
mkdir -p data
mkdir -p data/stats1  
seqkit stat *.fastq.gz >data/stats1/seqkit_stats.txt

```

## Mothur mapping file
The mapping files for use with `mothur` pipeline can be generated automatically if using the whole dataset. However, if you are interested in just a fraction of the files, you can create it manually and specify what to include.


```{r}
library(tidyverse)
read_table("data/stats1/seqkit_stats.txt") %>% 
  mutate(sample_id = str_replace_all(file, ".*/", ""), .before=file) %>% 
  mutate(sample_id = str_replace_all(sample_id, "_\\d?.fastq.gz", "")) %>%
  filter(str_detect(file, "_1" )) %>% 
  mutate(file2 =file, .after = file) %>% 
  mutate(file2 = str_replace_all(file, "_1.fastq.gz", "_2.fastq.gz")) %>%
  distinct() %>% 
  group_by(sample_id) %>%
  filter(num_seqs <65000) %>% 
  arrange(num_seqs) %>% 
  ungroup() %>%
  select(sample_id, forward = file, reverse = file2) %>% 
  write_tsv("data/mothur_mapping_file.tsv")
  # saveRDS("RDataRDS/mothur_mapping_file.rds")

read_tsv("data/mothur_mapping_file.tsv") %>% 
  as.data.frame()

```

## Metadata for Mothur pipeline
- Mothur pipeline expects the design (metadata) file to have column headers. 
- We will extract only the desired number of samples.
- The first column header should be **group**.
- For more detail click [here](https://mothur.org/wiki/miseq_sop/).

```{r}
read_tsv("data/mothur_mapping_file.tsv") %>% 
  select(sample_id) %>% 
  inner_join(., readRDS("RDataRDS/metadata.rds"), by = "sample_id") %>% 
  select(group = sample_id, isolate) %>%
  write_tsv("data/mothur_metadata.tsv")
  # saveRDS("RDataRDS/mothur_metadata.rds")

read_tsv("data/mothur_metadata.tsv") %>% 
  as.data.frame()
```

## QIIME2 mapping file

```{r}
library(tidyverse)
read_table("data/stats1/seqkit_stats.txt") %>% 
  mutate(sample_id = str_replace_all(file, ".*/", ""), .before=file) %>% 
  mutate(sample_id = str_replace_all(sample_id, "_\\d?.fastq.gz", "")) %>%
  filter(str_detect(file, "_1" )) %>% 
  mutate(file2 =file, .after = file) %>% 
  mutate(file2 = str_replace_all(file, "_1.fastq.gz", "_2.fastq.gz")) %>%
  distinct() %>% 
  group_by(sample_id) %>%
  filter(num_seqs <65000) %>% 
  arrange(num_seqs) %>%
  ungroup() %>% 
  mutate(file = str_replace_all(file, "SRR", "$PWD/SRR")) %>%
  mutate(file2 = str_replace_all(file2, "SRR", "$PWD/SRR")) %>%
  select("sample-id" = sample_id, "forward-absolute-filepath" = file, "reverse-absolute-filepath" = file2) %>% 
  write_tsv("data/q2-pe-33-manifest.tsv")
  # saveRDS("RDataRDS/q2-pe-33-manifest.rds")
  
read_tsv("data/q2-pe-33-manifest.tsv") %>% 
  as.data.frame()
```

## Metadata for QIIME2 pipeline
- The TSV format is recommended for QIIME2 metadata. For more details click [here](https://docs.qiime2.org/2022.2/tutorials/metadata/).
- The first column is sample identifier i.e. `sample-id` NOT with underscore like `sample_id`.
- The command below will replace `sample_id` in the metadata to `sample-id`.

```{r}
read_tsv("data/q2-pe-33-manifest.tsv") %>% 
  select(sample_id = "sample-id") %>% 
  inner_join(., readRDS("RDataRDS/metadata.rds"), by = "sample_id") %>% 
  rename("sample-id" = sample_id) %>%
  write_tsv("data/q2-metadata.tsv")
  # saveRDS("RDataRDS/q2-metadata.rds")

read_tsv("data/q2-metadata.tsv") %>% 
  as.data.frame()
```




<!--chapter:end:01f_mapping_files.Rmd-->

# (APPENDIX) APPENDIX {-}

# Saved Data Objects

```{r include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## RDS Format for Individual Object
```{r}

rds <- list.files(path="./RDataRDS", pattern = "*.rds", full.names = TRUE)
rds

```

## RData Format for Multiple Objects
```{r}

rdata <- list.files(path="./RDataRDS", pattern = "*.RData", full.names = TRUE)
rdata

```


## CSV or TSV Format Files
```{r}

list.files(path="./data", pattern = "*.csv|tsv", full.names = TRUE)

```


```{r eval=FALSE, include=FALSE}
## All-in-One Input-Output Data
lsdata("RDataRDS/saved_objects.RData")
```

## How to reload RDS or RData into R environment
```{block, type="tmbarrowF", echo=TRUE}

- RDS format e.g. foo.rds
  - foo <- readRDS("RDataRDS/foo.rds")

- RData format e.g. foo.RData
  - load("RDataRDS/foo.RData", verbose = TRUE)

- List objects in RData
  - lsdata("foo.RData") 

```


# Software and Packages

## Basic dependencies
* `r R.version.string`
* `tidyverse` (v. `r packageVersion("tidyverse")`)
* `knitr` (v. `r packageVersion("knitr")`)
* `rmarkdown` (v. `r packageVersion("rmarkdown")`)
* `bookdown` (v. `r packageVersion("bookdown")`)

<!-- * `ggpubr` (v. `r packageVersion("ggpubr")`) -->
<!-- * `downlit` (v. `r packageVersion("downlit")`) -->
<!-- * `phyloseq` (v. `r packageVersion("phyloseq")`) -->
<!-- * `ape` (v. `r packageVersion("ape")`) -->
<!-- * `ggtext` (v. `r packageVersion("ggtext")`) -->
<!-- * `dendextend` (v. `r packageVersion("dendextend")`) -->
<!-- * `metagMisc` (v. `r packageVersion("metagMisc")`) -->
<!-- * `cgwtools` (v. `r packageVersion("cgwtools")`) -->

## Available on machine used 
```{r}
sessionInfo()

```




<!--chapter:end:99_appendix.Rmd-->

`r if (knitr:::is_html_output()) '
# References {-}
'`


<!--chapter:end:999-references.Rmd-->

